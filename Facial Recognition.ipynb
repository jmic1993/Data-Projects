{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition Using Python and Open CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through the process of developing a model that can distinguish between the faces of Barrack Obama and Joe Biden. The main tool we use is the cv2 library, which contains a number of useful functions relating to image processing, face detection, and face recognition. \n",
    "\n",
    "I consulted a number of tutorials to figure out how to use the Open CV library to split a video into frames, detect and recognize faces, and rewrite images locally. Some that I found particularly useful are [here](http://eyalarubas.com/face-detection-and-recognition.html), [here](http://cbarker.net/opencv/), and [here](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html). \n",
    "\n",
    "First we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "faceDet = cv2.CascadeClassifier(\"/Users/Jason/anaconda/pkgs/opencv3-3.1.0-py35_0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "faceDet2 = cv2.CascadeClassifier(\"/Users/Jason/anaconda/pkgs/opencv3-3.1.0-py35_0/share/OpenCV/haarcascades/haarcascade_frontalface_alt2.xml\")\n",
    "faceDet3 = cv2.CascadeClassifier(\"/Users/Jason/anaconda/pkgs/opencv3-3.1.0-py35_0/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml\")\n",
    "faceDet4 = cv2.CascadeClassifier(\"/Users/Jason/anaconda/pkgs/opencv3-3.1.0-py35_0/share/OpenCV/haarcascades/haarcascade_frontalface_alt_tree.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures used here are all frames from various interviews and speeches featuring Obama or Biden. In a separate script, I downloaded videos of them, separated those videos into individual frames using the cv2 library, and used a built in face detector to crop each frame to contain only the relevant face. The code below creates a training set consisting of these frames (represented as 350x350 matrices), and the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = os.listdir('/Users/Jason/Desktop/Machine Learning/Facial Recognition/Pictures for Vid')\n",
    "del(names[names.index('.DS_Store')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y, name_vec, n = [], [], [], 0\n",
    "\n",
    "for name in names:\n",
    "    files = os.listdir('Final Pics for Vid/%s' %name)\n",
    "    for file in files:\n",
    "        if file != \".DS_Store\":\n",
    "            filename = 'Final Pics for Vid/%s' %name + '/' + file\n",
    "            picture = cv2.imread(filename) # Get each picture \n",
    "            gray = cv2.cvtColor(picture, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
    "            X_current = np.asarray(gray, dtype=np.uint8) # Append resulting matrix to training set \n",
    "            X.append(X_current) \n",
    "            Y.append(n) # Append training labels\n",
    "            name_vec.append(name)\n",
    "    n = n + 1\n",
    "    \n",
    "    \n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use cv2's built in LBPH (Local Binary Pattern Histogram) function to train a classifier on the two datasets. This method extracts features by seeing how dark each pixel is compared to its neighbors. It has the advantage of training within a minute or two (vs. 20 + minutes for other methods such as an eigenface classifier), while still performing fairly well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recognizer = cv2.face.createLBPHFaceRecognizer()\n",
    "recognizer.train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## See how the classifier performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('/Users/Jason/anaconda/pkgs/opencv3-3.1.0-py35_0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = os.listdir(\"Video/Frames\")\n",
    "del images[images.index('.DS_Store')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go through each frame of our test data (a video containing both Obama and Biden). We use cv2's built in face detector to find all faces in the image. Then we predict which person the face belongs to using our model, draw a rectangle around that face, and label it according to our prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/jenkins/miniconda/1/x64/conda-bld/work/opencv-3.1.0/modules/imgproc/src/color.cpp:7456: error: (-215) scn == 3 || scn == 4 in function ipp_cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-54c04d0fb042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"frame\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Video/Frames/%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/jenkins/miniconda/1/x64/conda-bld/work/opencv-3.1.0/modules/imgproc/src/color.cpp:7456: error: (-215) scn == 3 || scn == 4 in function ipp_cvtColor\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(images)):\n",
    "    image = \"frame\" + str(i) + \".jpg\"\n",
    "    img = cv2.imread('Video/Frames/%s' %image) # Read in each frame \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) # Detect all faces\n",
    "    for (x,y,w,h) in faces: \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # Draw rectangle around each face\n",
    "        roi_gray = gray[y:y+h, x:x+w] # Convert the cropped face to grayscale, predict whose it is w/ classifier\n",
    "        roi_gray = cv2.resize(roi_gray, (350, 350))\n",
    "        pred = recognizer.predict(roi_gray)\n",
    "        if pred == 0:\n",
    "            name = \"Biden\"\n",
    "        else:\n",
    "            name = \"Obama\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img,name,(x,y + h + 20), font, .5, (255,0,0)) # Write predicted name under image\n",
    "        num = str(count)\n",
    "        if len(num) == 1:\n",
    "            num = \"00\" + num\n",
    "        elif len(num) == 2:\n",
    "            num = \"0\" + num\n",
    "        cv2.imwrite(\"Video/Final_Frames/image%s\" %num + '.jpg', img) # Write edited frame out\n",
    "        \n",
    "    count += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting who the faces in our test data belong to, we can concatenate our labelled frames into a final video. The easiest way to do this is probably with [ffmpeg](http://trac.ffmpeg.org/wiki/Slideshow).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"Video/Final_Frames/LBHP2.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"Video/Final_Frames/LBHP2.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting video shows that our model worked pretty well. However, there are also some limitations that are evident. \n",
    "\n",
    "First of all, Biden's face is not always detected, as it is often not looking directly into the camera. There are also several instances where objects we don't want to recognize are flagged as faces. The most obvious case of this is the statue at the top-left corner of the screen. Additionally, there are a few frames where other objects (a pillar, the folder in Obama's hand, etc...) are flagged as faces. \n",
    "\n",
    "This seems to be more of a fault with the face detector than our actual model. One should be aware of the difficulties inherent in classifying faces when they are at irregular angles. However, once a face is detected, our classifier seems to do a pretty good job.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
